{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a8ef6dd",
      "metadata": {},
      "source": [
        "# Time-Series Forecasting: LSTM and Transformer (Questions 3 & 4)\n",
        "\n",
        "We forecast daily mean temperature (\"meantemp\") using sliding windows of 10 past values to predict the next value. The notebook first trains a vanilla LSTM (Q3) and then a Transformer-based encoder (Q4) on the provided Daily Climate Time Series data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699c1fb9",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Data lives in `data-climate/`. The workflow:\n",
        "1. Load train+test CSVs, keep only `meantemp`.\n",
        "2. Scale with `MinMaxScaler`.\n",
        "3. Create sequences of length 10 -> 11th target.\n",
        "4. Train/evaluate LSTM.\n",
        "5. Train/evaluate Transformer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81e416fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install dependencies if running in a fresh environment\n",
        "# !pip install -q torch torchvision scikit-learn matplotlib pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de00674",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "seed = 1337\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc79333",
      "metadata": {},
      "source": [
        "## Data loading and preprocessing (Q3a, Q3b)\n",
        "- Drop all columns except `meantemp`.\n",
        "- Build sequences of length 10 -> next value.\n",
        "- Scale values with `MinMaxScaler` (fit on the full series).\n",
        "- No shuffling or additional splitting is required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50b85b22",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = Path('data-climate')\n",
        "train_path = DATA_DIR / 'DailyDelhiClimateTrain.csv'\n",
        "test_path = DATA_DIR / 'DailyDelhiClimateTest.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# Keep only mean temperature and stack train + test for more signal\n",
        "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "meantemp_series = full_df['meantemp'].astype(float).reset_index(drop=True)\n",
        "print(f'Total samples: {len(meantemp_series)}')\n",
        "\n",
        "# Scale\n",
        "scaler = MinMaxScaler()\n",
        "scaled_all = scaler.fit_transform(meantemp_series.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "def build_sequences(values, window=10):\n",
        "    X, y = [], []\n",
        "    for i in range(len(values) - window):\n",
        "        X.append(values[i:i + window])\n",
        "        y.append(values[i + window])\n",
        "    return np.stack(X), np.array(y)\n",
        "\n",
        "WINDOW = 10\n",
        "X_scaled, y_scaled = build_sequences(scaled_all, window=WINDOW)\n",
        "print('Sequence array shape:', X_scaled.shape, y_scaled.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2354ed71",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset + DataLoader\n",
        "torch_X = torch.tensor(X_scaled, dtype=torch.float32).unsqueeze(-1)  # (N, window, 1)\n",
        "torch_y = torch.tensor(y_scaled, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "sequence_ds = SequenceDataset(torch_X, torch_y)\n",
        "batch_size = 64\n",
        "loader = DataLoader(sequence_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Quick visual of the raw series\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(meantemp_series.values, label='meantemp')\n",
        "plt.title('Daily mean temperature')\n",
        "plt.xlabel('Day index')\n",
        "plt.ylabel('Temp (C)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99c1aea3",
      "metadata": {},
      "source": [
        "## Vanilla LSTM model (Q3c)\n",
        "- Input: sequence of 10 values (1 feature each).\n",
        "- Output: next value.\n",
        "- Loss: MSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19fa5ce0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TempLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size=64, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=1,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        last = out[:, -1, :]\n",
        "        return self.head(last)\n",
        "\n",
        "\n",
        "def train_regression(model, loader, epochs=50, lr=1e-3, clip=1.0):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    history = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb.squeeze(-1)) if preds.ndim == 2 else criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "        epoch_loss = running_loss / len(loader.dataset)\n",
        "        history.append(epoch_loss)\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f'Epoch {epoch:03d} loss={epoch_loss:.6f}')\n",
        "    return history, criterion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d6136c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "lstm_model = TempLSTM(hidden_size=96, num_layers=2, dropout=0.2).to(device)\n",
        "lstm_history, lstm_criterion = train_regression(lstm_model, loader, epochs=60, lr=2e-3)\n",
        "\n",
        "plt.figure(figsize=(7, 3))\n",
        "plt.plot(lstm_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training loss (MSE)')\n",
        "plt.title('LSTM training loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359d8ffa",
      "metadata": {},
      "source": [
        "## LSTM evaluation (Q3d)\n",
        "We compare the predicted series (one-step ahead using the true previous 10 points) to the ground truth on the original scale.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f06523b",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_series(model, data_tensor):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for i in range(0, len(data_tensor), batch_size):\n",
        "        xb = data_tensor[i:i + batch_size].to(device)\n",
        "        out = model(xb).cpu().squeeze(-1)\n",
        "        preds.append(out)\n",
        "    return torch.cat(preds, dim=0)\n",
        "\n",
        "lstm_scaled_preds = predict_series(lstm_model, torch_X)\n",
        "lstm_preds = scaler.inverse_transform(lstm_scaled_preds.numpy().reshape(-1, 1)).flatten()\n",
        "lstm_targets = scaler.inverse_transform(y_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "mse_lstm = mean_squared_error(lstm_targets, lstm_preds)\n",
        "print(f'LSTM MSE (original scale): {mse_lstm:.4f}')\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(lstm_targets, label='Actual')\n",
        "plt.plot(lstm_preds, label='Predicted', alpha=0.8)\n",
        "plt.xlabel('Sample index')\n",
        "plt.ylabel('Mean temp (C)')\n",
        "plt.title('LSTM: actual vs predicted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2add84c",
      "metadata": {},
      "source": [
        "## Transformer-based model (Q4)\n",
        "A lightweight Transformer encoder processes the 10-step sequence. We reuse the same preprocessing and loader.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4346ae6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TempTransformer(nn.Module):\n",
        "    def __init__(self, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(1, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_model, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        encoded = self.encoder(x)\n",
        "        last = encoded[:, -1, :]\n",
        "        return self.head(last)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3015cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer_model = TempTransformer(d_model=64, nhead=4, num_layers=3, dim_feedforward=160, dropout=0.1).to(device)\n",
        "trans_history, trans_criterion = train_regression(transformer_model, loader, epochs=70, lr=2e-3)\n",
        "\n",
        "plt.figure(figsize=(7, 3))\n",
        "plt.plot(trans_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training loss (MSE)')\n",
        "plt.title('Transformer training loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4dde75c",
      "metadata": {},
      "source": [
        "## Transformer evaluation and comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1992f48b",
      "metadata": {},
      "outputs": [],
      "source": [
        "trans_scaled_preds = predict_series(transformer_model, torch_X)\n",
        "trans_preds = scaler.inverse_transform(trans_scaled_preds.numpy().reshape(-1, 1)).flatten()\n",
        "\n",
        "mse_trans = mean_squared_error(lstm_targets, trans_preds)\n",
        "print(f'Transformer MSE (original scale): {mse_trans:.4f}')\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(lstm_targets, label='Actual')\n",
        "plt.plot(trans_preds, label='Predicted', alpha=0.8)\n",
        "plt.xlabel('Sample index')\n",
        "plt.ylabel('Mean temp (C)')\n",
        "plt.title('Transformer: actual vs predicted')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "import pandas as pd\n",
        "comparison = pd.DataFrame([\n",
        "    {\"model\": \"LSTM\", \"mse\": mse_lstm},\n",
        "    {\"model\": \"Transformer\", \"mse\": mse_trans},\n",
        "])\n",
        "comparison\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
