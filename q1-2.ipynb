{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a9a53870",
      "metadata": {},
      "source": [
        "# Fingers Count Classification (Questions 1 & 2)\n",
        "\n",
        "This notebook trains two models on the Kaggle fingers counting dataset stored in `data-fingers/`: (1) a custom CNN, and (2) a ResNet18 trained from scratch. The code handles preprocessing directly from the flat image files whose filenames encode the label.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1aa7709",
      "metadata": {},
      "source": [
        "## Setup\n",
        "- Paths point to the provided `train` and `test` folders.\n",
        "- Uncomment the install cell below if you run on a fresh environment.\n",
        "- Everything else is pure PyTorch/torchvision/Sklearn + Matplotlib.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd489299",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment if you need to install the dependencies\n",
        "# !pip install -q torch torchvision torchaudio scikit-learn matplotlib seaborn pandas numpy pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0f16ff89",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4191d2e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reproducibility helpers\n",
        "\n",
        "def set_seed(seed: int = 1337) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "BASE_DIR = Path('data-fingers')\n",
        "TRAIN_DIR = BASE_DIR / 'train'\n",
        "TEST_DIR = BASE_DIR / 'test'\n",
        "assert TRAIN_DIR.exists() and TEST_DIR.exists(), 'Dataset folders not found'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01914076",
      "metadata": {},
      "source": [
        "## Data preprocessing\n",
        "The raw train/test folders are flat: filenames look like `abcd_3L.png`, where the digit is the label. We parse the digit and build datasets/dataloaders without reorganizing files on disk. Data augmentations are applied to the training split only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa107b9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "LABEL_RE = re.compile(r\"_(\\d)[LR]\\.png$\", flags=re.IGNORECASE)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "def parse_label(path: Path) -> int:\n",
        "    match = LABEL_RE.search(path.name)\n",
        "    if not match:\n",
        "        raise ValueError(f'Cannot parse label from {path.name}')\n",
        "    return int(match.group(1))\n",
        "\n",
        "\n",
        "class FingersDataset(Dataset):\n",
        "    def __init__(self, root, indices=None, transform=None):\n",
        "        self.root = Path(root)\n",
        "        self.transform = transform\n",
        "        samples = [(p, parse_label(p)) for p in sorted(self.root.glob('*.png'))]\n",
        "        if indices is not None:\n",
        "            samples = [samples[i] for i in indices]\n",
        "        if len(samples) == 0:\n",
        "            raise RuntimeError(f'No samples found in {root}')\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        with Image.open(path) as img:\n",
        "            img = img.convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "def build_transforms(image_size: int):\n",
        "    train_tfms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(12),\n",
        "        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.03),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "    ])\n",
        "    eval_tfms = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "    ])\n",
        "    return train_tfms, eval_tfms\n",
        "\n",
        "\n",
        "def make_dataloaders(image_size=160, batch_size=64, val_split=0.15, num_workers=2):\n",
        "    train_tfms, eval_tfms = build_transforms(image_size)\n",
        "    base_dataset = FingersDataset(TRAIN_DIR)\n",
        "\n",
        "    # Deterministic train/val split\n",
        "    total = len(base_dataset)\n",
        "    val_len = max(1, int(total * val_split))\n",
        "    indices = torch.randperm(total, generator=torch.Generator().manual_seed(42)).tolist()\n",
        "    val_idx = indices[:val_len]\n",
        "    train_idx = indices[val_len:]\n",
        "\n",
        "    train_ds = FingersDataset(TRAIN_DIR, indices=train_idx, transform=train_tfms)\n",
        "    val_ds = FingersDataset(TRAIN_DIR, indices=val_idx, transform=eval_tfms)\n",
        "    test_ds = FingersDataset(TEST_DIR, transform=eval_tfms)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, val_loader, test_loader, train_ds, val_ds, test_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1d8e73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick sanity check on label parsing and a few samples\n",
        "train_loader, val_loader, test_loader, train_ds, val_ds, test_ds = make_dataloaders(image_size=128, batch_size=32)\n",
        "print(f\"Train images: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
        "\n",
        "imgs, labels = next(iter(train_loader))\n",
        "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
        "for ax, img, lbl in zip(axes.flatten(), imgs[:8], labels[:8]):\n",
        "    img_np = img.permute(1, 2, 0).numpy() * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)\n",
        "    img_np = np.clip(img_np, 0, 1)\n",
        "    ax.imshow(img_np)\n",
        "    ax.set_title(f'label {lbl.item()}')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25d82b3",
      "metadata": {},
      "source": [
        "## Model architectures\n",
        "We start with a compact custom CNN, then swap in a torchvision ResNet18 (randomly initialized) while keeping the same preprocessing and hyperparameters (aside from resizing to 224 for ResNet18).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0ff90f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, pool=False):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        layers = [\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        if pool:\n",
        "            layers.append(nn.MaxPool2d(2))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class FingerCNN(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            ConvBlock(3, 32, pool=True),\n",
        "            ConvBlock(32, 64, pool=True),\n",
        "            ConvBlock(64, 128, pool=True),\n",
        "            ConvBlock(128, 256, pool=True),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.35),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def make_resnet18(num_classes=6):\n",
        "    model = models.resnet18(weights=None)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b53a9a",
      "metadata": {},
      "source": [
        "## Training & evaluation helpers\n",
        "- CrossEntropy loss + AdamW optimizer.\n",
        "- Cosine annealing LR by default; adjust below as needed.\n",
        "- We keep the best validation checkpoint in memory for testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35e4003",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return total_loss / total, total_correct / total\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return total_loss / total, total_correct / total\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=12, lr=3e-4, weight_decay=1e-4):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "    best_state = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "        scheduler.step()\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print(f\"Epoch {epoch:02d}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} train_acc={train_acc:.3f} val_acc={val_acc:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return model, history, criterion\n",
        "\n",
        "\n",
        "def plot_history(history, title='Training history'):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    axes[0].plot(epochs, history['train_loss'], label='train')\n",
        "    axes[0].plot(epochs, history['val_loss'], label='val')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Loss')\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].plot(epochs, history['train_acc'], label='train')\n",
        "    axes[1].plot(epochs, history['val_acc'], label='val')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].set_ylim(0, 1)\n",
        "    axes[1].set_title('Accuracy')\n",
        "    axes[1].legend()\n",
        "    fig.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluate_on_test(model, loader, criterion, label_names=None):\n",
        "    model.eval()\n",
        "    all_true, all_pred = [], []\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            all_true.extend(labels.cpu().tolist())\n",
        "            all_pred.extend(preds.cpu().tolist())\n",
        "    avg_loss = total_loss / len(all_true)\n",
        "    acc = accuracy_score(all_true, all_pred)\n",
        "    rec = recall_score(all_true, all_pred, average='macro')\n",
        "    cm = confusion_matrix(all_true, all_pred, labels=list(range(6)))\n",
        "\n",
        "    print(f\"Test loss: {avg_loss:.4f}, accuracy: {acc:.4f}, macro recall: {rec:.4f}\")\n",
        "    print(classification_report(all_true, all_pred, target_names=label_names))\n",
        "    return all_true, all_pred, cm\n",
        "\n",
        "\n",
        "def plot_confusion(cm, classes):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed73798",
      "metadata": {},
      "source": [
        "## Train custom CNN (Question 1)\n",
        "Adjust the hyperparameters below to trade off runtime vs accuracy. The defaults give a solid baseline while keeping the training short on CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e85ab02",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "cnn_image_size = 160\n",
        "batch_size = 64\n",
        "num_epochs = 12\n",
        "learning_rate = 3e-4\n",
        "weight_decay = 1e-4\n",
        "\n",
        "# Data\n",
        "train_loader, val_loader, test_loader, *_ = make_dataloaders(\n",
        "    image_size=cnn_image_size, batch_size=batch_size, val_split=0.15, num_workers=2\n",
        ")\n",
        "\n",
        "# Model\n",
        "cnn_model = FingerCNN(num_classes=6).to(device)\n",
        "cnn_model, cnn_history, cnn_criterion = train_model(\n",
        "    cnn_model, train_loader, val_loader, epochs=num_epochs, lr=learning_rate, weight_decay=weight_decay\n",
        ")\n",
        "plot_history(cnn_history, title='Custom CNN')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "494606af",
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = [str(i) for i in range(6)]\n",
        "cnn_true, cnn_pred, cnn_cm = evaluate_on_test(cnn_model, test_loader, cnn_criterion, label_names=classes)\n",
        "plot_confusion(cnn_cm, classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb0ae6b",
      "metadata": {},
      "source": [
        "## Train ResNet18 from scratch (Question 2)\n",
        "We keep all preprocessing and hyperparameters the same, changing only the architecture and the input resize (224x224 is the canonical size for ResNet18).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc1cb28",
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_image_size = 224\n",
        "\n",
        "train_loader_r, val_loader_r, test_loader_r, *_ = make_dataloaders(\n",
        "    image_size=resnet_image_size, batch_size=batch_size, val_split=0.15, num_workers=2\n",
        ")\n",
        "\n",
        "resnet_model = make_resnet18(num_classes=6).to(device)\n",
        "resnet_model, resnet_history, resnet_criterion = train_model(\n",
        "    resnet_model, train_loader_r, val_loader_r, epochs=num_epochs, lr=learning_rate, weight_decay=weight_decay\n",
        ")\n",
        "plot_history(resnet_history, title='ResNet18')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc2008d",
      "metadata": {},
      "outputs": [],
      "source": [
        "res_true, res_pred, res_cm = evaluate_on_test(resnet_model, test_loader_r, resnet_criterion, label_names=classes)\n",
        "plot_confusion(res_cm, classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac35cd87",
      "metadata": {},
      "source": [
        "## Model comparison\n",
        "Overlay validation accuracy curves to compare convergence, and summarize the test metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03da9770",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(cnn_history['val_acc'], label='Custom CNN')\n",
        "plt.plot(resnet_history['val_acc'], label='ResNet18')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Val accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.title('Validation accuracy comparison')\n",
        "plt.show()\n",
        "\n",
        "summary = pd.DataFrame([\n",
        "    {\n",
        "        'model': 'Custom CNN',\n",
        "        'test_accuracy': accuracy_score(cnn_true, cnn_pred),\n",
        "        'test_recall_macro': recall_score(cnn_true, cnn_pred, average='macro'),\n",
        "    },\n",
        "    {\n",
        "        'model': 'ResNet18',\n",
        "        'test_accuracy': accuracy_score(res_true, res_pred),\n",
        "        'test_recall_macro': recall_score(res_true, res_pred, average='macro'),\n",
        "    },\n",
        "])\n",
        "summary\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml_training_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
